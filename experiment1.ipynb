{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhas-bvp/session12/blob/master/experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c81c64d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3203f26d-3c0f-4d62-902b-209053239fb7"
      },
      "source": [
        "# Solving for residual std scaling issue\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Causal Self-Attention mechanism for GPT.\n",
        "    Implements multi-head attention with causal masking to ensure tokens can only attend to previous tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the causal self-attention layer.\n",
        "\n",
        "        Args:\n",
        "            config: GPTConfig object containing model hyperparameters\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Ensure embedding dimension is divisible by number of heads\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        # Single linear layer that computes Q, K, V for all heads simultaneously\n",
        "        # Output size is 3 * n_embd (for Q, K, V concatenated)\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "\n",
        "        # Output projection layer to combine attention outputs\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # Flag for special weight initialization (scaled by layer depth)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "\n",
        "        # Store hyperparameters\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        # Create causal mask: lower triangular matrix to prevent attending to future tokens\n",
        "        # This is registered as a buffer (not a parameter, so it won't be updated during training)\n",
        "        causal_mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"bias\", causal_mask.view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through causal self-attention.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (B, T, C) where B=batch, T=sequence length, C=embedding dim\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (B, T, C) after attention and projection\n",
        "        \"\"\"\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # Step 1: Compute Q, K, V for all heads in one go\n",
        "        # c_attn outputs (B, T, 3*C), we split it into Q, K, V each of size (B, T, C)\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        # Step 2: Reshape and transpose for multi-head attention\n",
        "        # Split embedding dimension across heads: C = n_head * head_size\n",
        "        # Reshape to (B, T, n_head, head_size) then transpose to (B, n_head, T, head_size)\n",
        "        # This allows parallel computation across all heads\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # Step 3: Compute attention scores\n",
        "        # Q @ K^T gives (B, nh, T, T) attention scores\n",
        "        # Scale by sqrt(head_size) to prevent large values before softmax\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "        # Step 4: Apply causal mask (set future positions to -inf so they become 0 after softmax)\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "\n",
        "        # Step 5: Apply softmax to get attention probabilities\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        # Step 6: Apply attention weights to values\n",
        "        # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = att @ v\n",
        "\n",
        "        # Step 7: Concatenate all heads back together\n",
        "        # Transpose back to (B, T, nh, hs) then reshape to (B, T, C)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        # Step 8: Apply output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron (feed-forward network) used in transformer blocks.\n",
        "    Implements a 2-layer MLP with GELU activation and 4x expansion ratio.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the MLP layer.\n",
        "\n",
        "        Args:\n",
        "            config: GPTConfig object containing model hyperparameters\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # First linear layer: expands from n_embd to 4*n_embd (expansion ratio of 4)\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "\n",
        "        # GELU activation function (Gaussian Error Linear Unit)\n",
        "        # Uses tanh approximation for efficiency\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "\n",
        "        # Second linear layer: projects back from 4*n_embd to n_embd\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        # Flag for special weight initialization (scaled by layer depth)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the MLP.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (B, T, C)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (B, T, C) after MLP transformation\n",
        "        \"\"\"\n",
        "        # Step 1: Expand dimension (B, T, C) -> (B, T, 4*C)\n",
        "        x = self.c_fc(x)\n",
        "\n",
        "        # Step 2: Apply GELU activation\n",
        "        x = self.gelu(x)\n",
        "\n",
        "        # Step 3: Project back to original dimension (B, T, 4*C) -> (B, T, C)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block containing self-attention and MLP with residual connections.\n",
        "    Implements pre-norm architecture: LayerNorm before attention/MLP, then residual connection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the transformer block.\n",
        "\n",
        "        Args:\n",
        "            config: GPTConfig object containing model hyperparameters\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Layer normalization before attention (pre-norm architecture)\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        # Causal self-attention layer\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "\n",
        "        # Layer normalization before MLP\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        # Feed-forward MLP\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the transformer block.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (B, T, C)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (B, T, C) after attention and MLP with residuals\n",
        "        \"\"\"\n",
        "        # Pre-norm attention with residual connection\n",
        "        # Apply LayerNorm, then attention, then add residual\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "\n",
        "        # Pre-norm MLP with residual connection\n",
        "        # Apply LayerNorm, then MLP, then add residual\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    \"\"\"\n",
        "    Configuration dataclass for GPT model hyperparameters.\n",
        "    Default values match GPT-2 small model (124M parameters).\n",
        "    \"\"\"\n",
        "    block_size: int = 1024  # Maximum sequence length (context window)\n",
        "    vocab_size: int = 50257  # Vocabulary size: 50,000 BPE merges + 256 byte tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12  # Number of transformer blocks (layers)\n",
        "    n_head: int = 12  # Number of attention heads\n",
        "    n_embd: int = 768  # Embedding dimension (hidden size)\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT (Generative Pre-trained Transformer) model.\n",
        "    Implements a decoder-only transformer architecture for language modeling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the GPT model.\n",
        "\n",
        "        Args:\n",
        "            config: GPTConfig object containing model hyperparameters\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Build transformer architecture\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),  # Token embeddings\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),  # Position embeddings\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # Stack of transformer blocks\n",
        "            ln_f = nn.LayerNorm(config.n_embd),  # Final layer normalization\n",
        "        ))\n",
        "\n",
        "        # Language modeling head: projects embeddings to vocabulary logits\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight sharing: share weights between token embedding and output projection\n",
        "        # This reduces parameters and improves training stability\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Initialize all weights using custom initialization scheme\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"\n",
        "        Custom weight initialization for GPT model.\n",
        "        Uses scaled initialization for residual connections to maintain variance.\n",
        "\n",
        "        Args:\n",
        "            module: PyTorch module to initialize\n",
        "        \"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Base standard deviation for weight initialization\n",
        "            std = 0.02\n",
        "\n",
        "            # For residual projection layers (marked with NANGPT_SCALE_INIT),\n",
        "            # scale down initialization by sqrt(2*n_layer) to prevent variance explosion\n",
        "            # This is important for deep networks with residual connections\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "\n",
        "            # Initialize weights from normal distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "            # Initialize biases to zero if they exist\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            # Initialize embedding weights from normal distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the GPT model.\n",
        "\n",
        "        Args:\n",
        "            idx: Input token indices of shape (B, T) where B=batch size, T=sequence length\n",
        "            targets: Optional target token indices of shape (B, T) for computing loss\n",
        "\n",
        "        Returns:\n",
        "            logits: Model predictions of shape (B, T, vocab_size)\n",
        "            loss: Cross-entropy loss if targets provided, else None\n",
        "        \"\"\"\n",
        "        # Get batch size and sequence length\n",
        "        B, T = idx.size()\n",
        "\n",
        "        # Ensure sequence doesn't exceed maximum block size\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        # Step 1: Create position indices [0, 1, 2, ..., T-1]\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
        "\n",
        "        # Step 2: Get position embeddings of shape (T, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "\n",
        "        # Step 3: Get token embeddings of shape (B, T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "\n",
        "        # Step 4: Combine token and position embeddings (add them together)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Step 5: Forward through all transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        # Step 6: Apply final layer normalization\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        # Step 7: Project to vocabulary logits of shape (B, T, vocab_size)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Step 8: Compute loss if targets are provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Reshape logits and targets to (B*T, vocab_size) and (B*T,) for cross-entropy\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"\n",
        "        Load pretrained GPT-2 model weights from HuggingFace.\n",
        "        Converts HuggingFace's GPT-2 weights to match our model architecture.\n",
        "\n",
        "        Args:\n",
        "            model_type: One of 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
        "\n",
        "        Returns:\n",
        "            GPT model with pretrained weights loaded\n",
        "        \"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # Map model type to architecture hyperparameters\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),   # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),   # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),   # 1558M params\n",
        "        }[model_type]\n",
        "\n",
        "        # GPT-2 models always use these fixed values\n",
        "        config_args['vocab_size'] = 50257  # Always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024   # Always 1024 for GPT model checkpoints\n",
        "\n",
        "        # Create a fresh GPT model with the correct architecture\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "\n",
        "        # Get state dict from our model (keys we need to populate)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        # Remove attention bias (it's a buffer, not a parameter)\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
        "\n",
        "        # Load pretrained model from HuggingFace\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # Filter HuggingFace state dict keys (remove buffers we don't need)\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # Ignore buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]  # Ignore mask buffer\n",
        "\n",
        "        # These layers use Conv1D in HuggingFace but Linear in our model, so weights need transposing\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "        # Verify we have matching number of parameters\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "\n",
        "        # Copy weights from HuggingFace model to our model\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # Special case: transpose Conv1D weights to Linear weights\n",
        "                # HuggingFace uses Conv1D which stores weights as (out_features, in_features)\n",
        "                # Our Linear layers expect (out_features, in_features) but need transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())  # Transpose when copying\n",
        "            else:\n",
        "                # Standard case: direct copy (shapes should match)\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n",
        "\n",
        "# Device selection: automatically choose the best available device\n",
        "# Priority: CUDA (GPU) > MPS (Apple Silicon) > CPU\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'  # NVIDIA GPU\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"  # Apple Silicon GPU (Metal Performance Shaders)\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "# This ensures consistent results across runs\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# Generation parameters (for text generation, currently unused due to early exit)\n",
        "num_return_sequences = 5  # Number of sequences to generate\n",
        "max_length = 30  # Maximum length of generated sequences\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "    \"\"\"\n",
        "    Lightweight data loader for GPT training.\n",
        "    Loads text data, tokenizes it, and provides batches for training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, B, T):\n",
        "        \"\"\"\n",
        "        Initialize the data loader.\n",
        "\n",
        "        Args:\n",
        "            B: Batch size (number of sequences per batch)\n",
        "            T: Sequence length (context window size)\n",
        "        \"\"\"\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # Step 1: Load text data from file into memory\n",
        "        with open('data/input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Step 2: Initialize GPT-2 tokenizer\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "        # Step 3: Tokenize the entire text corpus\n",
        "        tokens = enc.encode(text)\n",
        "\n",
        "        # Step 4: Convert to PyTorch tensor for efficient batching\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "        # Track current position in the token sequence for sequential batching\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        \"\"\"\n",
        "        Get the next batch of training data.\n",
        "        Creates input-target pairs where targets are inputs shifted by one position.\n",
        "\n",
        "        Returns:\n",
        "            x: Input tokens of shape (B, T)\n",
        "            y: Target tokens of shape (B, T) - same as x but shifted by 1 position\n",
        "        \"\"\"\n",
        "        B, T = self.B, self.T\n",
        "\n",
        "        # Extract a buffer of B*T+1 tokens (need +1 for target shift)\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "\n",
        "        # Create input-target pairs:\n",
        "        # x = tokens [0, 1, 2, ..., B*T-1]\n",
        "        # y = tokens [1, 2, 3, ..., B*T] (shifted by 1 for next-token prediction)\n",
        "        x = (buf[:-1]).view(B, T)  # inputs: shape (B, T)\n",
        "        y = (buf[1:]).view(B, T)   # targets: shape (B, T)\n",
        "\n",
        "        # Advance position for next batch\n",
        "        self.current_position += B * T\n",
        "\n",
        "        # Reset to beginning if we've reached the end of the data\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, device='cpu'):\n",
        "    \"\"\"\n",
        "    Load a saved model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Path to the checkpoint file (.pt)\n",
        "        device: Device to load the model on ('cpu', 'cuda', 'mps')\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded GPT model\n",
        "        checkpoint: Dictionary containing checkpoint information\n",
        "    \"\"\"\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Create model with saved config\n",
        "    config = checkpoint['config']\n",
        "    model = GPT(config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"Loaded model from step {checkpoint['step']} with loss {checkpoint['loss']:.4f}\")\n",
        "    return model, checkpoint\n",
        "\n",
        "\n",
        "# Initialize GPT model with default configuration (GPT-2 small architecture)\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)  # Move model to selected device (CPU/GPU)\n",
        "\n",
        "# Optimized training hyperparameters\n",
        "# Increased batch size and sequence length for better gradient estimates and longer context\n",
        "BATCH_SIZE = 16  # Increased from 4 for better gradient estimates\n",
        "SEQ_LENGTH = 128  # Increased from 32 for longer context understanding\n",
        "LEARNING_RATE = 6e-4  # Slightly higher initial LR for faster convergence\n",
        "WEIGHT_DECAY = 0.1  # Weight decay for regularization\n",
        "MAX_STEPS = 5000  # Increased from 50 for better convergence\n",
        "WARMUP_STEPS = 100  # Warmup steps for learning rate scheduling\n",
        "GRAD_CLIP = 1.0  # Gradient clipping threshold to prevent exploding gradients\n",
        "\n",
        "# Initialize data loader with optimized batch size and sequence length\n",
        "train_loader = DataLoaderLite(B=BATCH_SIZE, T=SEQ_LENGTH)\n",
        "\n",
        "# Initialize optimizer: AdamW with optimized hyperparameters\n",
        "# betas=(0.9, 0.95) are standard for transformer training\n",
        "# weight_decay helps with regularization\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "# Learning rate scheduler with warmup and cosine annealing\n",
        "def get_lr(it):\n",
        "    # Warmup phase: linearly increase LR from 0 to LEARNING_RATE\n",
        "    if it < WARMUP_STEPS:\n",
        "        return LEARNING_RATE * (it + 1) / WARMUP_STEPS\n",
        "    # Cosine annealing: decay LR following cosine curve\n",
        "    progress = (it - WARMUP_STEPS) / (MAX_STEPS - WARMUP_STEPS)\n",
        "    return LEARNING_RATE * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "# Create checkpoint directory for saving models\n",
        "checkpoint_dir = 'data/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Training loop with optimizations\n",
        "print(f\"Starting training with batch_size={BATCH_SIZE}, seq_length={SEQ_LENGTH}\")\n",
        "print(f\"Total steps: {MAX_STEPS}, Warmup steps: {WARMUP_STEPS}\")\n",
        "print(f\"Checkpoints will be saved to: {checkpoint_dir}/\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_step = 0\n",
        "losses = []\n",
        "\n",
        "for step in range(MAX_STEPS):\n",
        "    # Step 1: Get next batch of input-target pairs\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)  # Move data to device\n",
        "\n",
        "    # Step 2: Update learning rate with scheduler\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # Step 3: Zero out gradients from previous iteration\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Step 4: Forward pass: compute logits and loss\n",
        "    logits, loss = model(x, y)\n",
        "\n",
        "    # Step 5: Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Step 6: Gradient clipping to prevent exploding gradients\n",
        "    # This is crucial for training stability in transformers\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "\n",
        "    # Step 7: Update model parameters using computed gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # Step 8: Track loss\n",
        "    loss_val = loss.item()\n",
        "    losses.append(loss_val)\n",
        "\n",
        "    # Step 9: Save model checkpoint if this is the best loss so far\n",
        "    if loss_val < best_loss:\n",
        "        best_loss = loss_val\n",
        "        best_step = step\n",
        "\n",
        "        # Save checkpoint with model state, optimizer state, and training info\n",
        "        checkpoint = {\n",
        "            'step': step,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss_val,\n",
        "            'best_loss': best_loss,\n",
        "            'config': model.config,\n",
        "            'learning_rate': lr,\n",
        "        }\n",
        "\n",
        "        # Save best model checkpoint\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f'✓ Saved best model checkpoint at step {step} with loss {loss_val:.4f}')\n",
        "\n",
        "    # Step 10: Print progress periodically\n",
        "    if step % 100 == 0 or step < 10:\n",
        "        print(f'step {step:5d} | lr: {lr:.2e} | loss: {loss_val:.4f} | best: {best_loss:.4f} (step {best_step})')\n",
        "\n",
        "    # Early stopping if loss becomes very small (optional)\n",
        "    if loss_val < 0.01:\n",
        "        print(f\"Loss converged to {loss_val:.4f} at step {step}\")\n",
        "        break\n",
        "\n",
        "# Save final model checkpoint\n",
        "final_checkpoint = {\n",
        "    'step': step,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': losses[-1],\n",
        "    'best_loss': best_loss,\n",
        "    'best_step': best_step,\n",
        "    'config': model.config,\n",
        "    'learning_rate': get_lr(step),\n",
        "    'total_steps': len(losses),\n",
        "}\n",
        "final_checkpoint_path = os.path.join(checkpoint_dir, 'final_model.pt')\n",
        "torch.save(final_checkpoint, final_checkpoint_path)\n",
        "\n",
        "# Print training summary\n",
        "print(\"-\" * 60)\n",
        "print(f\"Training completed!\")\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "print(f\"Best loss: {best_loss:.4f} (achieved at step {best_step})\")\n",
        "print(f\"Average loss (last 100 steps): {sum(losses[-100:])/len(losses[-100:]):.4f}\")\n",
        "print(f\"Best model saved to: {os.path.join(checkpoint_dir, 'best_model.pt')}\")\n",
        "print(f\"Final model saved to: {os.path.join(checkpoint_dir, 'final_model.pt')}\")\n",
        "\n",
        "import sys; sys.exit(0)  # Exit early (generation code below is not executed)\n",
        "\n",
        "# Text generation code (currently not executed due to early exit above)\n",
        "# This implements top-k sampling for generating text\n",
        "\n",
        "torch.manual_seed(42)  # Set seed for reproducible generation\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Generate tokens until we reach max_length\n",
        "while x.size(1) < max_length:\n",
        "    # Forward pass: get logits for next token prediction\n",
        "    # Use no_grad() to disable gradient computation (faster, uses less memory)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0]  # (B, T, vocab_size) - logits for all positions\n",
        "\n",
        "        # Step 1: Extract logits at the last position (we only need the next token)\n",
        "        logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "\n",
        "        # Step 2: Convert logits to probabilities using softmax\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Step 3: Top-k sampling: keep only top 50 most likely tokens\n",
        "        # This reduces the search space and improves generation quality\n",
        "        # topk_probs: (B, 50) - probabilities of top-k tokens\n",
        "        # topk_indices: (B, 50) - indices of top-k tokens\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "        # Step 4: Sample one token from the top-k probabilities\n",
        "        # multinomial samples according to the probability distribution\n",
        "        # Note: multinomial doesn't require probabilities to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1)  # (B, 1) - index into top-k list\n",
        "\n",
        "        # Step 5: Get the actual token index from topk_indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix)  # (B, 1) - actual token index\n",
        "\n",
        "        # Step 6: Append the new token to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# Decode and print generated text for each sequence in the batch\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()  # Convert tensor to list\n",
        "    decoded = enc.decode(tokens)  # Decode tokens back to text\n",
        "    print(\">\", decoded)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 165 batches\n",
            "Starting training with batch_size=16, seq_length=128\n",
            "Total steps: 5000, Warmup steps: 100\n",
            "Checkpoints will be saved to: data/checkpoints/\n",
            "------------------------------------------------------------\n",
            "✓ Saved best model checkpoint at step 0 with loss 10.9671\n",
            "step     0 | lr: 6.00e-06 | loss: 10.9671 | best: 10.9671 (step 0)\n",
            "✓ Saved best model checkpoint at step 1 with loss 10.8050\n",
            "step     1 | lr: 1.20e-05 | loss: 10.8050 | best: 10.8050 (step 1)\n",
            "✓ Saved best model checkpoint at step 2 with loss 10.5279\n",
            "step     2 | lr: 1.80e-05 | loss: 10.5279 | best: 10.5279 (step 2)\n",
            "✓ Saved best model checkpoint at step 3 with loss 10.1284\n",
            "step     3 | lr: 2.40e-05 | loss: 10.1284 | best: 10.1284 (step 3)\n",
            "✓ Saved best model checkpoint at step 4 with loss 9.8261\n",
            "step     4 | lr: 3.00e-05 | loss: 9.8261 | best: 9.8261 (step 4)\n",
            "✓ Saved best model checkpoint at step 5 with loss 9.5432\n",
            "step     5 | lr: 3.60e-05 | loss: 9.5432 | best: 9.5432 (step 5)\n",
            "✓ Saved best model checkpoint at step 6 with loss 9.2413\n",
            "step     6 | lr: 4.20e-05 | loss: 9.2413 | best: 9.2413 (step 6)\n",
            "✓ Saved best model checkpoint at step 7 with loss 9.1960\n",
            "step     7 | lr: 4.80e-05 | loss: 9.1960 | best: 9.1960 (step 7)\n",
            "✓ Saved best model checkpoint at step 8 with loss 8.9672\n",
            "step     8 | lr: 5.40e-05 | loss: 8.9672 | best: 8.9672 (step 8)\n",
            "✓ Saved best model checkpoint at step 9 with loss 8.8223\n",
            "step     9 | lr: 6.00e-05 | loss: 8.8223 | best: 8.8223 (step 9)\n",
            "✓ Saved best model checkpoint at step 10 with loss 8.6923\n",
            "✓ Saved best model checkpoint at step 11 with loss 8.3341\n",
            "✓ Saved best model checkpoint at step 15 with loss 8.2464\n",
            "✓ Saved best model checkpoint at step 17 with loss 8.0080\n",
            "✓ Saved best model checkpoint at step 19 with loss 7.8810\n",
            "✓ Saved best model checkpoint at step 22 with loss 7.6915\n",
            "✓ Saved best model checkpoint at step 25 with loss 7.6835\n",
            "✓ Saved best model checkpoint at step 26 with loss 7.6295\n",
            "✓ Saved best model checkpoint at step 27 with loss 7.6048\n",
            "✓ Saved best model checkpoint at step 28 with loss 7.3657\n",
            "✓ Saved best model checkpoint at step 29 with loss 7.1118\n",
            "✓ Saved best model checkpoint at step 31 with loss 7.0694\n",
            "✓ Saved best model checkpoint at step 32 with loss 6.8423\n",
            "✓ Saved best model checkpoint at step 33 with loss 6.6412\n",
            "✓ Saved best model checkpoint at step 34 with loss 6.5635\n",
            "✓ Saved best model checkpoint at step 35 with loss 6.4251\n",
            "✓ Saved best model checkpoint at step 38 with loss 6.3926\n",
            "✓ Saved best model checkpoint at step 42 with loss 6.3061\n",
            "✓ Saved best model checkpoint at step 43 with loss 6.2066\n",
            "✓ Saved best model checkpoint at step 53 with loss 6.1073\n",
            "✓ Saved best model checkpoint at step 57 with loss 6.0194\n",
            "✓ Saved best model checkpoint at step 59 with loss 5.9048\n",
            "✓ Saved best model checkpoint at step 60 with loss 5.7048\n",
            "✓ Saved best model checkpoint at step 62 with loss 5.3512\n",
            "✓ Saved best model checkpoint at step 94 with loss 5.3414\n",
            "✓ Saved best model checkpoint at step 99 with loss 5.3281\n",
            "step   100 | lr: 6.00e-04 | loss: 5.3741 | best: 5.3281 (step 99)\n",
            "✓ Saved best model checkpoint at step 102 with loss 5.0785\n",
            "✓ Saved best model checkpoint at step 140 with loss 4.9283\n",
            "✓ Saved best model checkpoint at step 157 with loss 4.9147\n",
            "✓ Saved best model checkpoint at step 158 with loss 4.5023\n",
            "step   200 | lr: 5.99e-04 | loss: 4.8338 | best: 4.5023 (step 158)\n",
            "✓ Saved best model checkpoint at step 227 with loss 4.3190\n",
            "step   300 | lr: 5.98e-04 | loss: 4.4709 | best: 4.3190 (step 227)\n",
            "✓ Saved best model checkpoint at step 305 with loss 4.2797\n",
            "✓ Saved best model checkpoint at step 322 with loss 4.2235\n",
            "✓ Saved best model checkpoint at step 323 with loss 3.8096\n",
            "step   400 | lr: 5.94e-04 | loss: 4.3997 | best: 3.8096 (step 323)\n",
            "✓ Saved best model checkpoint at step 488 with loss 3.5142\n",
            "step   500 | lr: 5.90e-04 | loss: 4.7834 | best: 3.5142 (step 488)\n",
            "step   600 | lr: 5.85e-04 | loss: 4.8440 | best: 3.5142 (step 488)\n",
            "✓ Saved best model checkpoint at step 653 with loss 3.3455\n",
            "step   700 | lr: 5.78e-04 | loss: 4.4291 | best: 3.3455 (step 653)\n",
            "step   800 | lr: 5.70e-04 | loss: 3.6229 | best: 3.3455 (step 653)\n",
            "✓ Saved best model checkpoint at step 818 with loss 3.2009\n",
            "step   900 | lr: 5.61e-04 | loss: 4.2563 | best: 3.2009 (step 818)\n",
            "✓ Saved best model checkpoint at step 983 with loss 3.0648\n",
            "step  1000 | lr: 5.51e-04 | loss: 3.7158 | best: 3.0648 (step 983)\n",
            "step  1100 | lr: 5.40e-04 | loss: 4.0180 | best: 3.0648 (step 983)\n",
            "✓ Saved best model checkpoint at step 1148 with loss 2.8984\n",
            "step  1200 | lr: 5.28e-04 | loss: 3.6371 | best: 2.8984 (step 1148)\n",
            "step  1300 | lr: 5.16e-04 | loss: 3.6698 | best: 2.8984 (step 1148)\n",
            "✓ Saved best model checkpoint at step 1313 with loss 2.7874\n",
            "step  1400 | lr: 5.02e-04 | loss: 3.2766 | best: 2.7874 (step 1313)\n",
            "✓ Saved best model checkpoint at step 1478 with loss 2.6172\n",
            "step  1500 | lr: 4.87e-04 | loss: 3.2132 | best: 2.6172 (step 1478)\n",
            "step  1600 | lr: 4.72e-04 | loss: 3.7726 | best: 2.6172 (step 1478)\n",
            "✓ Saved best model checkpoint at step 1643 with loss 2.4584\n",
            "step  1700 | lr: 4.56e-04 | loss: 3.1275 | best: 2.4584 (step 1643)\n",
            "step  1800 | lr: 4.39e-04 | loss: 2.9635 | best: 2.4584 (step 1643)\n",
            "✓ Saved best model checkpoint at step 1808 with loss 2.2977\n",
            "step  1900 | lr: 4.21e-04 | loss: 2.7740 | best: 2.2977 (step 1808)\n",
            "✓ Saved best model checkpoint at step 1973 with loss 2.1450\n",
            "step  2000 | lr: 4.04e-04 | loss: 2.9383 | best: 2.1450 (step 1973)\n",
            "step  2100 | lr: 3.85e-04 | loss: 3.0567 | best: 2.1450 (step 1973)\n",
            "✓ Saved best model checkpoint at step 2138 with loss 1.9796\n",
            "step  2200 | lr: 3.67e-04 | loss: 2.6981 | best: 1.9796 (step 2138)\n",
            "step  2300 | lr: 3.48e-04 | loss: 2.1936 | best: 1.9796 (step 2138)\n",
            "✓ Saved best model checkpoint at step 2303 with loss 1.8262\n",
            "step  2400 | lr: 3.29e-04 | loss: 2.1545 | best: 1.8262 (step 2303)\n",
            "✓ Saved best model checkpoint at step 2468 with loss 1.6752\n",
            "step  2500 | lr: 3.10e-04 | loss: 1.9429 | best: 1.6752 (step 2468)\n",
            "✓ Saved best model checkpoint at step 2537 with loss 1.6407\n",
            "step  2600 | lr: 2.90e-04 | loss: 2.3618 | best: 1.6407 (step 2537)\n",
            "✓ Saved best model checkpoint at step 2633 with loss 1.4837\n",
            "step  2700 | lr: 2.71e-04 | loss: 1.7764 | best: 1.4837 (step 2633)\n",
            "✓ Saved best model checkpoint at step 2702 with loss 1.4429\n",
            "✓ Saved best model checkpoint at step 2798 with loss 1.2973\n",
            "step  2800 | lr: 2.52e-04 | loss: 1.9602 | best: 1.2973 (step 2798)\n",
            "✓ Saved best model checkpoint at step 2867 with loss 1.2692\n",
            "step  2900 | lr: 2.33e-04 | loss: 1.6324 | best: 1.2692 (step 2867)\n",
            "✓ Saved best model checkpoint at step 2963 with loss 1.1756\n",
            "step  3000 | lr: 2.15e-04 | loss: 1.4719 | best: 1.1756 (step 2963)\n",
            "✓ Saved best model checkpoint at step 3032 with loss 1.0291\n",
            "step  3100 | lr: 1.96e-04 | loss: 1.4123 | best: 1.0291 (step 3032)\n",
            "✓ Saved best model checkpoint at step 3128 with loss 0.9784\n",
            "✓ Saved best model checkpoint at step 3197 with loss 0.8614\n",
            "step  3200 | lr: 1.79e-04 | loss: 1.2894 | best: 0.8614 (step 3197)\n",
            "✓ Saved best model checkpoint at step 3293 with loss 0.8271\n",
            "step  3300 | lr: 1.61e-04 | loss: 1.4415 | best: 0.8271 (step 3293)\n",
            "✓ Saved best model checkpoint at step 3362 with loss 0.6765\n",
            "step  3400 | lr: 1.44e-04 | loss: 0.8841 | best: 0.6765 (step 3362)\n",
            "step  3500 | lr: 1.28e-04 | loss: 0.7846 | best: 0.6765 (step 3362)\n",
            "✓ Saved best model checkpoint at step 3527 with loss 0.5200\n",
            "step  3600 | lr: 1.13e-04 | loss: 0.6519 | best: 0.5200 (step 3527)\n",
            "✓ Saved best model checkpoint at step 3692 with loss 0.4150\n",
            "step  3700 | lr: 9.83e-05 | loss: 0.7122 | best: 0.4150 (step 3692)\n",
            "step  3800 | lr: 8.45e-05 | loss: 0.7118 | best: 0.4150 (step 3692)\n",
            "✓ Saved best model checkpoint at step 3857 with loss 0.3317\n",
            "step  3900 | lr: 7.16e-05 | loss: 0.6953 | best: 0.3317 (step 3857)\n",
            "step  4000 | lr: 5.96e-05 | loss: 0.4721 | best: 0.3317 (step 3857)\n",
            "✓ Saved best model checkpoint at step 4022 with loss 0.2593\n",
            "step  4100 | lr: 4.86e-05 | loss: 0.3662 | best: 0.2593 (step 4022)\n",
            "✓ Saved best model checkpoint at step 4187 with loss 0.2194\n",
            "step  4200 | lr: 3.86e-05 | loss: 0.4677 | best: 0.2194 (step 4187)\n",
            "step  4300 | lr: 2.97e-05 | loss: 0.4053 | best: 0.2194 (step 4187)\n",
            "✓ Saved best model checkpoint at step 4352 with loss 0.2015\n",
            "step  4400 | lr: 2.19e-05 | loss: 0.5071 | best: 0.2015 (step 4352)\n",
            "step  4500 | lr: 1.53e-05 | loss: 0.3487 | best: 0.2015 (step 4352)\n",
            "✓ Saved best model checkpoint at step 4517 with loss 0.1716\n",
            "step  4600 | lr: 9.81e-06 | loss: 0.4818 | best: 0.1716 (step 4517)\n",
            "step  4700 | lr: 5.53e-06 | loss: 0.3567 | best: 0.1716 (step 4517)\n",
            "step  4800 | lr: 2.46e-06 | loss: 0.4802 | best: 0.1716 (step 4517)\n",
            "step  4900 | lr: 6.16e-07 | loss: 0.4786 | best: 0.1716 (step 4517)\n",
            "------------------------------------------------------------\n",
            "Training completed!\n",
            "Final loss: 0.3453\n",
            "Best loss: 0.1716 (achieved at step 4517)\n",
            "Average loss (last 100 steps): 0.3894\n",
            "Best model saved to: data/checkpoints/best_model.pt\n",
            "Final model saved to: data/checkpoints/final_model.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}